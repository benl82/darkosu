{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59252ae3",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53846134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8251a",
   "metadata": {},
   "source": [
    "SQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e0dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+pymysql://user1:greenvoid@127.0.0.1/osu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48286588",
   "metadata": {},
   "source": [
    "Pandas DF creation, sql dump import. Will probably have to revamp this in the future due to crazy runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d65b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cz = 50000 # chunksize\n",
    "connection = engine.connect().execution_options(stream_results = True)\n",
    "\n",
    "def chunkr(str1):\n",
    "    query = \"SELECT * FROM \" + str1\n",
    "    bigdf = pd.DataFrame(); lbog = []\n",
    "    for df in pd.read_sql(query, connection, chunksize = cz): lbog.append(df)\n",
    "    bigdf = pd.concat(lbog).reset_index(drop = True); return bigdf\n",
    "\n",
    "osu_beatmap_difficulty = chunkr(\"osu_beatmap_difficulty\")\n",
    "osu_beatmap_difficulty_attribs = chunkr(\"osu_beatmap_difficulty_attribs\")\n",
    "osu_beatmaps = chunkr(\"osu_beatmaps\")\n",
    "osu_beatmap_performance_blacklist = chunkr(\"osu_beatmap_performance_blacklist\")\n",
    "osu_beatmap_failtimes = chunkr(\"osu_beatmap_failtimes\")\n",
    "osu_beatmapsets = chunkr(\"osu_beatmapsets\")\n",
    "osu_counts = chunkr(\"osu_counts\")\n",
    "osu_difficulty_attribs = chunkr(\"osu_difficulty_attribs\")\n",
    "osu_scores_mania_high = chunkr(\"osu_scores_mania_high\")\n",
    "osu_user_beatmap_playcount = chunkr(\"osu_user_beatmap_playcount\")\n",
    "osu_user_stats_mania = chunkr(\"osu_user_stats_mania\")\n",
    "sample_users = chunkr(\"sample_users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c462ce",
   "metadata": {},
   "source": [
    "Imports 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c1cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hotting failtimes' fail-type as there's only 2 - 'fail' and 'exit'\n",
    "temp = pd.get_dummies(osu_beatmap_failtimes['type'])\n",
    "osu_beatmap_failtimes = osu_beatmap_failtimes.drop('type', axis=1)\n",
    "osu_beatmap_failtimes = osu_beatmap_failtimes.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "396fc5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40108770\n"
     ]
    }
   ],
   "source": [
    "# The MySQL read speed chunkwise is absolutely terrible, there must be some way to optimize it without sacrificing 10GB ram lol\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "from datasets import list_datasets\n",
    "# It might be possible to inner join the osu_beatmap_difficulty and osu_beatmap_difficulty attribs dataframes\n",
    "# del merged_osu_beatmap_difficulty\n",
    "# del merged_osu_beatmaps\n",
    "merged_osu_beatmap_difficulty = pd.merge(osu_beatmap_difficulty, osu_beatmap_difficulty_attribs, how = 'inner',\n",
    "                                        left_on = ['beatmap_id', 'mode', 'mods'],\n",
    "                                        right_on = ['beatmap_id', 'mode', 'mods'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ed2fcaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging again with osu_beatmaps\n",
    "# I have MemoryErrors so I'm just gonna do a subset of 5K\n",
    "train1 = osu_beatmaps.head(200)\n",
    "test1 = osu_beatmaps.iloc[5001:5200]\n",
    "merged_osu_beatmaps = pd.merge(merged_osu_beatmap_difficulty, train1, how = 'inner',\n",
    "                               on = 'beatmap_id')\n",
    "train_osu_beatmaps = pd.merge(merged_osu_beatmap_difficulty, test1, how = 'inner',\n",
    "                               on = 'beatmap_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b601fe4",
   "metadata": {},
   "source": [
    "Custom Datasets, built for osu scores - predicting score from beatmap_id, user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "498b8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.io import read_image\n",
    "from datasets import Dataset\n",
    "\n",
    "# Transforming some of the data sets. This section is a work in progress\n",
    "# Beatmap_id is going to get right merged with a few factors here.\n",
    "# From osu_beatmaps - beatmap_id, hit_length, countNormal, countSlider, countSpinner, diff_overall, difficultyrating\n",
    "# This dataset seems to be from 2012 / 2013, so the maps might be all converts. Will probably have to download some more sets.\n",
    "# From osu_beatmap_difficulty_attribs - Try to match the beatmap_id and mods columns to each other, and get diff_unified.\n",
    "    # I am not sure what diff_unified means.\n",
    "\n",
    "# From osu_user_stats_mania - We have count300, count100, count50, countMiss, accuracy, playcount, replay_popularity,\n",
    "    # rank_score, rank_score_index\n",
    "# From osu_user_beatmap_playcount - we can get the playcount of a map from user id and beatmap id.\n",
    "\n",
    "# Mostly I'm just going to try to get the numerical values and avoid redundancy - timestamp's definitely aren't relevant here\n",
    "\n",
    "#dataset_osu_beatmap_difficulty = Dataset.from_pandas(merged_osu_beatmap_difficulty).with_format(\"torch\",\n",
    "#                            columns = ['beatmap_id', 'mode', 'mods', 'diff_unified', 'attrib_id', 'value'])\n",
    "# AR and CS don't affect mania so I'm going to leave them out\n",
    "merged_osu_beatmaps['temp'] = merged_osu_beatmaps[['hit_length', 'diff_unified',\n",
    "                                       'countTotal', 'countNormal', 'countSlider',\n",
    "                                      'countSpinner', 'diff_drain', 'diff_overall']].values.tolist()\n",
    "train_osu_beatmaps['temp'] = train_osu_beatmaps[['hit_length', 'diff_unified',\n",
    "                                       'countTotal', 'countNormal', 'countSlider',\n",
    "                                      'countSpinner', 'diff_drain', 'diff_overall']].values.tolist()\n",
    "dataset_osu_beatmaps = Dataset.from_pandas(merged_osu_beatmaps).with_format(\"torch\",\n",
    "                            columns = ['temp', 'difficultyrating'])\n",
    "dataset_osu_beatmaps_t = Dataset.from_pandas(train_osu_beatmaps).with_format(\"torch\",\n",
    "                            columns = ['temp', 'difficultyrating'])\n",
    "# The blacklist isn't relevant either\n",
    "# dataset_osu_beatmap_performance_blacklist = Dataset.from_pandas(osu_beatmap_performance_blacklist).with_format(\"torch\")\n",
    "'''\n",
    "dataset_osu_beatmap_failtimes = Dataset.from_pandas(osu_beatmap_failtimes).with_format(\"torch\")\n",
    "dataset_osu_beatmapsets = Dataset.from_pandas(osu_beatmapsets).with_format(\"torch\")\n",
    "dataset_osu_counts = Dataset.from_pandas(osu_counts).with_format(\"torch\")\n",
    "dataset_osu_difficulty_attribs = Dataset.from_pandas(osu_difficulty_attribs).with_format(\"torch\")\n",
    "dataset_osu_scores_mania_high = Dataset.from_pandas(osu_scores_mania_high).with_format(\"torch\")\n",
    "dataset_osu_user_beatmap_playcount = Dataset.from_pandas(osu_user_beatmap_playcount).with_format(\"torch\")\n",
    "dataset_osu_user_stats_mania = Dataset.from_pandas(osu_user_stats_mania).with_format(\"torch\", \n",
    "                            columns = ['count300', 'count100', 'count50', 'countMiss', 'accuracy_new',\n",
    "                                      'replay_popularity', 'rank_score', 'rank_score_index'], output_all_columns = False)\n",
    "dataset_sample_users = Dataset.from_pandas(sample_users).with_format(\"torch\")\n",
    "'''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b5b20",
   "metadata": {},
   "source": [
    "Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b16934e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't even need this anymore, importing Datasets instead.\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset = load_dataset('dataset_osu_beatmap_difficulty', 'dataset_osu_beatmaps)\n",
    "dataloader = DataLoader(dataset_osu_beatmaps, batch_size = 100, shuffle = True)\n",
    "dataloader_t = DataLoader(dataset_osu_beatmaps_t, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079eed0",
   "metadata": {},
   "source": [
    "Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d87d4635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([100, 8])\n",
      "Labels batch shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "asdf = next(iter(dataloader))\n",
    "train_features = asdf['temp']\n",
    "train_labels = asdf['difficultyrating']\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147e001",
   "metadata": {},
   "source": [
    "Building Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9ac87f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0bad2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 1) # completely random lmao\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork(); model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "61c35b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2e025bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------\n",
      "loss: 8.889619 [    0/81000]\n",
      "loss: 0.512903 [10000/81000]\n",
      "loss: 0.430212 [20000/81000]\n",
      "loss: 0.340913 [30000/81000]\n",
      "loss: 0.452941 [40000/81000]\n",
      "loss: 0.421775 [50000/81000]\n",
      "loss: 0.510948 [60000/81000]\n",
      "loss: 0.477341 [70000/81000]\n",
      "loss: 0.608253 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.4%, Avg loss: 1.436587 \n",
      "\n",
      "Epoch 2\n",
      "------------------------\n",
      "loss: 0.572596 [    0/81000]\n",
      "loss: 0.478370 [10000/81000]\n",
      "loss: 0.466390 [20000/81000]\n",
      "loss: 0.381533 [30000/81000]\n",
      "loss: 0.499507 [40000/81000]\n",
      "loss: 0.540632 [50000/81000]\n",
      "loss: 0.461526 [60000/81000]\n",
      "loss: 0.420693 [70000/81000]\n",
      "loss: 0.372132 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.4%, Avg loss: 1.461388 \n",
      "\n",
      "Epoch 3\n",
      "------------------------\n",
      "loss: 0.541965 [    0/81000]\n",
      "loss: 0.315352 [10000/81000]\n",
      "loss: 0.393388 [20000/81000]\n",
      "loss: 0.402104 [30000/81000]\n",
      "loss: 0.428192 [40000/81000]\n",
      "loss: 0.426359 [50000/81000]\n",
      "loss: 0.492843 [60000/81000]\n",
      "loss: 0.350461 [70000/81000]\n",
      "loss: 0.364844 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 1.453649 \n",
      "\n",
      "Epoch 4\n",
      "------------------------\n",
      "loss: 0.348994 [    0/81000]\n",
      "loss: 0.500009 [10000/81000]\n",
      "loss: 0.534808 [20000/81000]\n",
      "loss: 0.409067 [30000/81000]\n",
      "loss: 0.412426 [40000/81000]\n",
      "loss: 0.407869 [50000/81000]\n",
      "loss: 0.371109 [60000/81000]\n",
      "loss: 0.352000 [70000/81000]\n",
      "loss: 0.370269 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 1.620049 \n",
      "\n",
      "Epoch 5\n",
      "------------------------\n",
      "loss: 0.312175 [    0/81000]\n",
      "loss: 0.613523 [10000/81000]\n",
      "loss: 0.288362 [20000/81000]\n",
      "loss: 0.454291 [30000/81000]\n",
      "loss: 0.372434 [40000/81000]\n",
      "loss: 0.381183 [50000/81000]\n",
      "loss: 0.398693 [60000/81000]\n",
      "loss: 0.435409 [70000/81000]\n",
      "loss: 0.476634 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.3%, Avg loss: 1.497128 \n",
      "\n",
      "Epoch 6\n",
      "------------------------\n",
      "loss: 0.284096 [    0/81000]\n",
      "loss: 0.471049 [10000/81000]\n",
      "loss: 0.493293 [20000/81000]\n",
      "loss: 0.555166 [30000/81000]\n",
      "loss: 0.608787 [40000/81000]\n",
      "loss: 0.461089 [50000/81000]\n",
      "loss: 0.461085 [60000/81000]\n",
      "loss: 0.491002 [70000/81000]\n",
      "loss: 0.449789 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 1.400148 \n",
      "\n",
      "Epoch 7\n",
      "------------------------\n",
      "loss: 0.619393 [    0/81000]\n",
      "loss: 0.541577 [10000/81000]\n",
      "loss: 0.303525 [20000/81000]\n",
      "loss: 0.343149 [30000/81000]\n",
      "loss: 0.473262 [40000/81000]\n",
      "loss: 0.316450 [50000/81000]\n",
      "loss: 0.605177 [60000/81000]\n",
      "loss: 0.521796 [70000/81000]\n",
      "loss: 0.414524 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 1.440812 \n",
      "\n",
      "Epoch 8\n",
      "------------------------\n",
      "loss: 0.482298 [    0/81000]\n",
      "loss: 0.536530 [10000/81000]\n",
      "loss: 0.527945 [20000/81000]\n",
      "loss: 0.460998 [30000/81000]\n",
      "loss: 0.469421 [40000/81000]\n",
      "loss: 0.516496 [50000/81000]\n",
      "loss: 0.441005 [60000/81000]\n",
      "loss: 0.470769 [70000/81000]\n",
      "loss: 0.560560 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.4%, Avg loss: 1.433693 \n",
      "\n",
      "Epoch 9\n",
      "------------------------\n",
      "loss: 0.583888 [    0/81000]\n",
      "loss: 0.529437 [10000/81000]\n",
      "loss: 0.324206 [20000/81000]\n",
      "loss: 0.401821 [30000/81000]\n",
      "loss: 0.332657 [40000/81000]\n",
      "loss: 0.457452 [50000/81000]\n",
      "loss: 0.355498 [60000/81000]\n",
      "loss: 0.418973 [70000/81000]\n",
      "loss: 0.522218 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 1.532613 \n",
      "\n",
      "Epoch 10\n",
      "------------------------\n",
      "loss: 0.504021 [    0/81000]\n",
      "loss: 0.564561 [10000/81000]\n",
      "loss: 0.471138 [20000/81000]\n",
      "loss: 0.412736 [30000/81000]\n",
      "loss: 0.412190 [40000/81000]\n",
      "loss: 0.604718 [50000/81000]\n",
      "loss: 0.256476 [60000/81000]\n",
      "loss: 0.444021 [70000/81000]\n",
      "loss: 0.528779 [80000/81000]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 1.501732 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def acc(pred, y):\n",
    "    n_items = tuple(y.size())[0]\n",
    "    n_correct = torch.sum((torch.abs(pred - y) < torch.abs(0.2 * y))) # Why don't I just define my own acc function to\n",
    "                                                                    # make me look like a genius?\n",
    "    acc = (n_correct.item() / n_items)\n",
    "    return acc\n",
    "\n",
    "def tloop(dataloader, model, loss, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        X, y = batch['temp'], batch['difficultyrating']\n",
    "        X = X.cuda(); y = y.cuda()\n",
    "        pred = model(X).cuda()\n",
    "        los = loss(pred, y).cuda()\n",
    "        optimizer.zero_grad(); los.backward(); model.float(); optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            los, current = los.item(), i * len(X)\n",
    "            print(f\"loss: {los:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def tloop_t(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            X, y = batch['temp'], batch['difficultyrating']\n",
    "            X = X.cuda(); y = y.cuda()\n",
    "            pred = model(X).cuda()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += acc(pred, y)\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n------------------------\")\n",
    "    tloop(dataloader, model, loss, optimizer)\n",
    "    tloop_t(dataloader_t, model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1009de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
